# from langchain_ollama import ChatOllama  # Import Ollama's integration with LangChain
# from langchain_community.vectorstores import Chroma
# from langchain.embeddings import OllamaEmbeddings  # Ollama embeddings if needed
# from text_to_doc import get_doc_chunks
# from web_crawler import get_data_from_website
# from prompt import get_prompt
# from langchain.chains import ConversationalRetrievalChain

# # Initialize Ollama model
# llm = ChatOllama(
#     model_name="llama-3.1",
#     temperature=0.7
# )

# embeddings = OllamaEmbeddings(
#     model="llama3",
# )

# def get_chroma_client():
#     """
#     Returns a Chroma vector store instance.
#     Uses Ollama embeddings for similarity search.

#     Returns:
#         langchain.vectorstores.chroma.Chroma: ChromaDB vector store instance.
#     """
#     embedding_function = embeddings  # Use Ollama embeddings here
#     return Chroma(
#         collection_name="website_data",
#         embedding_function=embedding_function,
#         persist_directory="data/chroma"
#     )

# def store_docs(url):
#     """
#     Retrieves data from a website, processes it into document chunks, and stores them in a vector store.

#     Args:
#         url (str): The URL of the website to retrieve data from.

#     Returns:
#         None
#     """
#     text, metadata = get_data_from_website(url)
#     docs = get_doc_chunks(text, metadata)
#     vector_store = get_chroma_client()
#     vector_store.add_documents(docs)
#     vector_store.persist()

# def make_chain():
#     """
#     Creates a chain of LangChain components using Ollama's Chat model and Chroma retriever.

#     Returns:
#         langchain.chains.ConversationalRetrievalChain: ConversationalRetrievalChain instance.
#     """
#     model = llm  # Using Ollama model here
#     vector_store = get_chroma_client()
#     prompt = get_prompt()

#     retriever = vector_store.as_retriever(search_type="mmr", verbose=True)

#     chain = ConversationalRetrievalChain.from_llm(
#         model,
#         retriever=retriever,
#         return_source_documents=True,
#         combine_docs_chain_kwargs=dict(prompt=prompt),
#         verbose=True,
#         rephrase_question=False,
#     )
#     return chain

# def get_response(question, organization_name, organization_info, contact_info):
#     """
#     Generates a response based on the input question.

#     Args:
#         question (str): The input question to generate a response for.
#         organization_name (str): The name of the organization.
#         organization_info (str): Information about the organization.
#         contact_info (str): Contact information for the organization.

#     Returns:
#         str: The response generated by the chain model.
#     """
#     chat_history = ""
#     chain = make_chain()
#     response = chain({
#         "question": question,
#         "chat_history": chat_history,
#         "organization_name": organization_name,
#         "contact_info": contact_info,
#         "organization_info": organization_info
#     })
#     return response['answer']





# from langchain_ollama import ChatOllama  # Import Ollama's integration with LangChain
# from langchain_community.vectorstores import Chroma
# from langchain.embeddings import OllamaEmbeddings  # Ollama embeddings if needed
# from text_to_doc import get_doc_chunks
# from web_crawler import get_data_from_website
# from prompt import get_prompt
# from langchain.chains import ConversationalRetrievalChain

# llm = ChatOllama(
#     model="llama3.1",
#     temperature=0,
#     # other params...
# )

# embeddings = OllamaEmbeddings(
#     model="llama3.1",  # Make sure the embedding model name is correct
# )

# def get_chroma_client():
#     """
#     Returns a Chroma vector store instance.
#     Uses Ollama embeddings for similarity search.

#     Returns:
#         langchain.vectorstores.chroma.Chroma: ChromaDB vector store instance.
#     """
#     embedding_function = embeddings  # Use Ollama embeddings here
#     return Chroma(
#         collection_name="website_data",
#         embedding_function=embedding_function,
#         persist_directory="data/chroma"  # Directory to persist the embeddings
#     )

# def store_docs(url):
#     """
#     Retrieves data from a website, processes it into document chunks, and stores them in a vector store.

#     Args:
#         url (str): The URL of the website to retrieve data from.

#     Returns:
#         None
#     """
#     text, metadata = get_data_from_website(url)  # Crawl website for text and metadata
#     docs = get_doc_chunks(text, metadata)  # Convert text into document chunks
#     vector_store = get_chroma_client()  # Initialize vector store client
#     vector_store.add_documents(docs)  # Add documents to the vector store
#     vector_store.persist()  # Persist the store to disk

# def make_chain():
#     """
#     Creates a chain of LangChain components using Ollama's Chat model and Chroma retriever.

#     Returns:
#         langchain.chains.ConversationalRetrievalChain: ConversationalRetrievalChain instance.
#     """
#     model = llm  # Use Ollama's LLaMA model for the chain
#     vector_store = get_chroma_client()  # Get the Chroma vector store
#     prompt = get_prompt()  # Get the prompt template

#     # Create a retriever with Maximum Marginal Relevance (MMR) to ensure diverse results
#     retriever = vector_store.as_retriever(search_type="mmr", verbose=True)

#     # Create the conversational retrieval chain
#     chain = ConversationalRetrievalChain.from_llm(
#         model,  # Ollama Chat model
#         retriever=retriever,
#         return_source_documents=True,  # Return source documents for context
#         combine_docs_chain_kwargs=dict(prompt=prompt),  # Include prompt template for combining docs
#         verbose=True,
#         rephrase_question=False,  # Avoid rephrasing the question
#     )
#     return chain

# def get_response(question, organization_name, organization_info, contact_info):
#     """
#     Generates a response based on the input question using the conversational retrieval chain.

#     Args:
#         question (str): The input question to generate a response for.
#         organization_name (str): The name of the organization.
#         organization_info (str): Information about the organization.
#         contact_info (str): Contact information for the organization.

#     Returns:
#         str: The response generated by the chain model.
#     """
#     chat_history = []  # Initialize chat history
#     chain = make_chain()  # Create the chain

#     # Execute the chain with relevant inputs
#     response = chain({
#         "question": question,
#         "chat_history": chat_history,  # Pass chat history (empty in this case)
#         "organization_name": organization_name,  # Organization details
#         "contact_info": contact_info,
#         "organization_info": organization_info
#     })
    
#     return response['answer']  # Return the answer generated by the model


# def add_text_to_vector_store(text, metadata=None):
#     """
#     Accepts a text input, processes it into document chunks, and adds it to the Chroma vector store.

#     Args:
#         text (str): The input text to add to the vector store.
#         metadata (dict, optional): Optional metadata associated with the text.

#     Returns:
#         None
#     """
#     docs = get_doc_chunks(text, metadata)  # Process text into document chunks
#     vector_store = get_chroma_client()  # Get Chroma vector store instance
#     vector_store.add_documents(docs)  # Add processed documents to vector store
#     vector_store.persist()  # Persist the vector store


from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEndpoint
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFaceHub
from dotenv import load_dotenv
import os

load_dotenv()

# ✅ Define HuggingFace Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
    
)

# ✅ Define HuggingFace LLM (text generation)

llm = HuggingFaceEndpoint(
    repo_id="google/flan-t5-base",
    temperature=0.7,
    max_new_tokens=256
)




# ✅ Create/Load Chroma Vector Store
def get_chroma_client():
    return Chroma(
        collection_name="website_data",
        embedding_function=embeddings,
        persist_directory="data/chroma"
    )

# ✅ Split input text into smaller chunks
def get_doc_chunks(text, metadata=None):
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.create_documents([text], metadatas=[metadata] if metadata else None)
    return chunks

# ✅ Add a text document to the vector store
def add_text_to_vector_store(text, metadata=None):
    docs = get_doc_chunks(text, metadata)
    vector_store = get_chroma_client()
    vector_store.add_documents(docs)
    vector_store.persist()

# ✅ Add a webpage's content to vector store
def store_docs(url):
    loader = WebBaseLoader(url)
    docs = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(docs)
    vector_store = get_chroma_client()
    vector_store.add_documents(texts)
    vector_store.persist()

# ✅ Build retrieval + generation chain
def make_chain():
    vector_store = get_chroma_client()
    retriever = vector_store.as_retriever(search_type="mmr", verbose=True)

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True
    )
    return chain

# ✅ Get answer from chatbot
def get_response(question, organization_name, organization_info, contact_info):
    chat_history = []

    chain = make_chain()
    response = chain({
        "question": question,
        "chat_history": chat_history,
        "organization": organization_name,
        "description": organization_info,
        "contact_info": contact_info
    })

    return response['answer']
